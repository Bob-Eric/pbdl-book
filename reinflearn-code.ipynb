{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aml7ksJPtCmf"
   },
   "source": [
    "# Controlling Burgers' Equation with Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B87Sa-fMYcOx"
   },
   "source": [
    "In the following, we will target inverse problems with Burgers equation as a testbed for reinforcement learning (RL). The setup is similar to the inverse problems previously targeted with differentiable physics (DP) training (cf. {doc}`diffphys-control`), and hence we'll also directly compare to these approaches below. Similar to before, Burgers equation is simple but non-linear with interesting dynamics, and hence a good starting point for RL experiments. In the following, the goal is to train a control force estimator network that should predict the forces needed to generate smooth transitions between two given states. \n",
    "\n",
    "## Overview\n",
    "\n",
    "Reinforcement learning describes an agent perceiving an environment and taking actions inside it. It aims at maximizing an accumulated sum of rewards, which it receives for those actions by the environment. Thus, the agent learns empirically which actions to take in different situations. _Proximal policy optimization_ [PPO](https://arxiv.org/abs/1707.06347v2) is a widely used reinforcement learning algorithm describing two neural networks: a policy model selecting actions for given observations and a value estimator network rating the reward potential of those states. These value estimates form the loss of the policy model, given by the change in reward potential by the chosen action.\n",
    "\n",
    "This notebook illustrates how PPO reinforcement learning can be applied to the described control problem of Burgers' equation. In comparison to the DP approach, the RL method does not have access to a differentiable physics solver, it is _model-free_. For the RL setup, this effectively means that we're able to pass gradients through the environment function, which is not always a given. \n",
    "\n",
    "However, the goal of the value estimator model is to compensate for this lack of a solver, as it tries to capture the long term effect of individual actions. Thus, an interesting question the following code example should answer is: can the model-free PPO reinforcement learning match the performance of the model-based DP training. We will compare this in terms of learning speed and the amount of required forces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHETNWlVtyWr"
   },
   "source": [
    "## Software installation\n",
    "\n",
    "This example uses the reinforcement learning framework [stable_baselines3](https://github.com/DLR-RM/stable-baselines3) and version 1.5.1 of the differentiable PDE solver [Î¦<sub>Flow</sub>](https://github.com/tum-pbs/PhiFlow). [PPO](https://arxiv.org/abs/1707.06347v2) was chosen as reinforcement learning algorithm.\n",
    "\n",
    "Additionally, a supervised control force estimator is trained as a performance baseline. This method was introduced by Holl et al. [\\(2020\\)](https://ge.in.tum.de/publications/2020-iclr-holl/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EDqLS_xz9B8"
   },
   "outputs": [],
   "source": [
    "!pip install stable-baselines3 phiflow==1.5.1\n",
    "!git clone https://github.com/Sh0cktr4p/PDE-Control-RL.git\n",
    "!git clone https://github.com/holl-/PDE-Control.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9t4odMH6pl1"
   },
   "source": [
    "Now we can import the necessary modules. Due to the scope of this example, there are quite a few modules to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UI1_mMnNQXrN"
   },
   "outputs": [],
   "source": [
    "import sys; sys.path.append('PDE-Control/src'); sys.path.append('PDE-Control-RL/src')\n",
    "import time, csv, os, shutil\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "from phi.flow import *\n",
    "import burgers_plots as bplt\n",
    "import matplotlib.pyplot as plt\n",
    "from envs.burgers_util import GaussianClash, GaussianForce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCUbc-sovPME"
   },
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSELidjsvRyd"
   },
   "source": [
    "At first we generate a dataset to train the CFE model on and to evaluate the performance of both approaches during and after training. The code below simulates 1000 cases (i.e. phiflow \"scenes\"), and keeps 100 of them as validation and test cases, respectively. The remaining 800 are used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUENHywEUVsu"
   },
   "outputs": [],
   "source": [
    "domain = Domain([32], box=box[0:1])     # Size and shape of the fields\n",
    "viscosity = 0.003\n",
    "step_count = 32                         # Trajectory length\n",
    "dt = 0.03\n",
    "diffusion_substeps = 1\n",
    "\n",
    "data_path = 'forced-burgers-clash'\n",
    "scene_count = 1000\n",
    "batch_size = 100\n",
    "\n",
    "train_range = range(200, 1000)\n",
    "val_range = range(100, 200)\n",
    "test_range = range(0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vEMdDJjAUeUv"
   },
   "outputs": [],
   "source": [
    "for batch_index in range(scene_count // batch_size):\n",
    "    scene = Scene.create(data_path, count=batch_size)\n",
    "    print(scene)\n",
    "    world = World()\n",
    "    u0 = BurgersVelocity(\n",
    "        domain, \n",
    "        velocity=GaussianClash(batch_size), \n",
    "        viscosity=viscosity, \n",
    "        batch_size=batch_size, \n",
    "        name='burgers'\n",
    "    )\n",
    "    u = world.add(u0, physics=Burgers(diffusion_substeps=diffusion_substeps))\n",
    "    force = world.add(FieldEffect(GaussianForce(batch_size), ['velocity']))\n",
    "    scene.write(world.state, frame=0)\n",
    "    for frame in range(1, step_count + 1):\n",
    "        world.step(dt=dt)\n",
    "        scene.write(world.state, frame=frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plZUZD_av3YH"
   },
   "source": [
    "## Reinforcement Learning Training\n",
    "\n",
    "Next we set up the RL environment.\n",
    "\n",
    "The reinforcement learning approach uses a dedicated value estimator network (the \"critic\") to predict the sum of rewards generated from a certain state. These are then used to update a policy network (the \"actor\") which, analogously to the control force estimator network of {doc}`diffphys-control` and the next section below, predicts the forces to control the simulation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Agc9EVUeUoY9"
   },
   "outputs": [],
   "source": [
    "from experiment import BurgersTraining\n",
    "\n",
    "n_envs = 10                         # On how many environments to train in parallel, load balancing\n",
    "final_reward_factor = step_count    # Penalty for not reaching the goal state\n",
    "steps_per_rollout = step_count * 10 # How many steps to collect per environment between agent updates\n",
    "n_epochs = 10                       # How many epochs to perform during each agent update\n",
    "rl_learning_rate = 1e-4             # Learning rate for agent updates\n",
    "rl_batch_size = 128                 # Batch size for agent updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4FKqSjwv9jR"
   },
   "source": [
    "To start training, we create a trainer object which manages the environment and the agent internally. Additionally, a directory for storing models, logs, and hyperparameters is created. This way, training can be continued at any later point using the same configuration. If the model folder specified in exp_name already exists, the agent within is loaded. Otherwise, a new agent is created.\n",
    "\n",
    "By default, an agent is stored at `PDE-Control-RL/networks/rl-models/bench`, and loaded if it exists. To generate a new model, replace the specified path with another.\n",
    "\n",
    "**TODO, explain PPO setup and environment**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjB_0vNKVCxe"
   },
   "outputs": [],
   "source": [
    "rl_trainer = BurgersTraining(\n",
    "    path='PDE-Control-RL/networks/rl-models/bench', # Replace this to train a new model\n",
    "    domain=domain,\n",
    "    viscosity=viscosity,\n",
    "    step_count=step_count,\n",
    "    dt=dt,\n",
    "    diffusion_substeps=diffusion_substeps,\n",
    "    n_envs=n_envs,\n",
    "    final_reward_factor=final_reward_factor,\n",
    "    steps_per_rollout=steps_per_rollout,\n",
    "    n_epochs=n_epochs,\n",
    "    learning_rate=rl_learning_rate,\n",
    "    batch_size=rl_batch_size,\n",
    "    data_path=data_path,\n",
    "    val_range=val_range,\n",
    "    test_range=test_range,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skE_zAdGwkM2"
   },
   "source": [
    "The following cell opens tensorboard inside the notebook to display the progress of the training. If a new model was created at a different location, please change the path to the location at which you stored your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DM8bVThNVF9Y"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir PDE-Control-RL/networks/rl-models/bench/tensorboard-log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nY5uq750wzsK"
   },
   "source": [
    "Now we are set up to start training the agent. The RL approach requires many iterations to explore the environment. Hence, the next cell typically takes multiple hours to execute (around 6h for 1000 rollouts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "laqpvrc7VxcW"
   },
   "outputs": [],
   "source": [
    "rl_trainer.train(n_rollouts=1000, save_freq=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WlqEvsOL7Rt"
   },
   "source": [
    "## RL Evaluation\n",
    "\n",
    "Let us take a glance what the results look like. \n",
    "\n",
    "**TODO, explain: source and target, show unmodified evolution, in comparison to controlled one**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y05aa5BjMVFZ"
   },
   "outputs": [],
   "source": [
    "rl_frames, _, _ = rl_trainer.infer_test_set_frames()\n",
    "\n",
    "index_in_set = 0    # Change this to display a reconstruction of another scene\n",
    "\n",
    "bplt.burgers_figure('Reinforcement Learning')\n",
    "for frame in range(0, step_count + 1):\n",
    "    plt.plot(rl_frames[frame][index_in_set,:], color=bplt.gradient_color(frame, step_count+1), linewidth=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**TODO, what do we see? briefly discuss: seems to work quite well already**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2E_sFqgo2SiU"
   },
   "source": [
    "## Differentiable Physics Training\n",
    "\n",
    "To classify the results of the reinforcement learning method, we now compare them to an approach using differentiable physics training. In contrast to the full approach from {doc}`diffphys-control` which includes a second _OP_ network, we aim for a direct control here. The OP network represents a separate \"physics-predictor\", which is omitted here for fairness with the RL version.\n",
    "\n",
    "The DP approach has access to the gradient data provided by the differentiable solver, making it possible to trace the loss over multiple timesteps and enabling the model to comprehend long term effects of generated forces better. The reinforcement learning algorithm, on the other hand, is not limited by training set size like the CFE approach, as new training samples are generated on policy. However, this also introduces additional simulation overhead during training, which can increase the time needed for convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2hqdeJb2GLJ"
   },
   "outputs": [],
   "source": [
    "from control.pde.burgers import BurgersPDE\n",
    "from control.control_training import ControlTraining\n",
    "from control.sequences import StaggeredSequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLcFuRBz0-Yf"
   },
   "source": [
    "The cell below sets up a model for training or to load an existing model checkpoint.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5s6-hSZp5CGb"
   },
   "outputs": [],
   "source": [
    "cfe_app = ControlTraining(\n",
    "    step_count,\n",
    "    BurgersPDE(domain, viscosity, dt),\n",
    "    datapath=data_path,\n",
    "    val_range=val_range,\n",
    "    train_range=train_range,\n",
    "    trace_to_channel=lambda trace: 'burgers_velocity',\n",
    "    obs_loss_frames=[],\n",
    "    trainable_networks=['CFE'],\n",
    "    sequence_class=StaggeredSequence,\n",
    "    batch_size=100,\n",
    "    view_size=20,\n",
    "    learning_rate=1e-3,\n",
    "    learning_rate_half_life=1000,\n",
    "    dt=dt\n",
    ").prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ReXUkzI1L3t"
   },
   "source": [
    "Now we can execute the model training. This cell might take long to execute, depending on the number of iterations (ca. 1.8h for 1000 iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blHHLaVS5jHA"
   },
   "outputs": [],
   "source": [
    "cfe_training_eval_data = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "cfe_training_iterations = 2000  # Change this to change training duration\n",
    "\n",
    "for epoch in range(cfe_training_iterations):\n",
    "    cfe_app.progress()\n",
    "    # Evaluate validation set at regular intervals to track learning progress\n",
    "    # Size of intervals determined by RL epoch count per iteration for accurate comparison\n",
    "    if epoch % n_epochs == 0:\n",
    "        f = cfe_app.infer_scalars(val_range)['Total Force'] / dt\n",
    "        cfe_training_eval_data.append((time.time() - start_time, epoch, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31B72FBR1pXr"
   },
   "source": [
    "We store the trained model and the validation performance with respect to iterations and wall time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzfckDMc8O__"
   },
   "outputs": [],
   "source": [
    "cfe_store_path = 'networks/cfe-models/bench'\n",
    "if not os.path.exists(cfe_store_path):\n",
    "    os.makedirs(cfe_store_path)\n",
    "\n",
    "# store training progress information\n",
    "with open(os.path.join(cfe_store_path, 'val_forces.csv'), 'at') as log_file:\n",
    "    logger = csv.DictWriter(log_file, ('time', 'epoch', 'forces'))\n",
    "    logger.writeheader()\n",
    "    for (t, e, f) in cfe_training_eval_data:\n",
    "        logger.writerow({'time': t, 'epoch': e, 'forces': f})\n",
    "\n",
    "cfe_checkpoint = cfe_app.save_model()\n",
    "shutil.move(cfe_checkpoint, cfe_store_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4r6sOh87B-1"
   },
   "source": [
    "Alternatively, run the cell below to load an existing network model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHEAxxjv-vDL"
   },
   "outputs": [],
   "source": [
    "cfe_path = 'PDE-Control-RL/networks/cfe-models/bench/checkpoint_00020000/'\n",
    "networks_to_load = ['OP2', 'OP4', 'OP8', 'OP16', 'OP32']\n",
    "\n",
    "cfe_app.load_checkpoints({net: cfe_path for net in networks_to_load})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8inOSSE0OMf"
   },
   "source": [
    "The next cell plots an example to show visually how well the DP-based model does. With this, we have an RL and a DP version, which we can compare in more detail in the next section.\n",
    "\n",
    "**TODO, like for RL above , show unmodified and controlled**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjYY0PXp1VDT"
   },
   "outputs": [],
   "source": [
    "cfe_frames = cfe_app.infer_all_frames(test_range)\n",
    "\n",
    "index_in_set = 1    # Change this to display a reconstruction of another scene\n",
    "\n",
    "bplt.burgers_figure('Supervised Control Force Estimator')\n",
    "for frame in range(0, step_count + 1):\n",
    "    plt.plot(cfe_frames[frame].burgers.velocity.data[index_in_set,:,0], color=bplt.gradient_color(frame, step_count+1), linewidth=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gA63vV4d2yko"
   },
   "source": [
    "## Comparison between RL and DP\n",
    "\n",
    "Next, the results of both methods are compared in terms of visual quality of the resulting trajectories as well as quantitatively via the amounf of generated forces. The latter provides insights about the performance of either approaches as both methods aspire to minimize this metric during training, and the task is trivially solved with by applying a huge force. Rather, an ideal solution takes into account the dynamics of the PDE to apply as little forces as possible. Hence, this metric is a very good one to measure how well the network has learned about the underlying physical environment (Burgers equation in this example).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuoxjQf8UVuF"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yh-xD2cG7d9A"
   },
   "source": [
    "### Trajectory Comparison\n",
    "\n",
    "To compare the resulting trajectories, we generate trajectories from the test set with either method. Also, we collect the ground truth simulations and the natural evolution of the test set fields.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EP8S7UC5_vQD"
   },
   "outputs": [],
   "source": [
    "rl_frames, gt_frames, unc_frames = rl_trainer.infer_test_set_frames()\n",
    "\n",
    "cfe_frames = cfe_app.infer_all_frames(test_range)\n",
    "cfe_frames = [s.burgers.velocity.data for s in cfe_frames]\n",
    "\n",
    "frames = {\n",
    "    (0, 0): ('Ground Truth', gt_frames),\n",
    "    (0, 1): ('Uncontrolled', unc_frames),\n",
    "    (1, 0): ('Reinforcement Learning', rl_frames),\n",
    "    (1, 1): ('Supervised Control Force Estimator', cfe_frames),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6DJ3J6jAkQ5"
   },
   "outputs": [],
   "source": [
    "index_in_set = 0    # Specifies which sample of the test set should be displayed\n",
    "\n",
    "def plot(axs, xy, title, field):\n",
    "    axs[xy].set_ylim(-2, 2)\n",
    "    axs[xy].set_xlabel('x')\n",
    "    axs[xy].set_ylabel('u(x)')\n",
    "    axs[xy].set_title(title)\n",
    "\n",
    "    label = 'Initial state in dark red, final state in dark blue'\n",
    "\n",
    "    for step_idx in range(0, step_count + 1):\n",
    "        color = bplt.gradient_color(step_idx, step_count+1)\n",
    "        axs[xy].plot(\n",
    "            field[step_idx][index_in_set].squeeze(), \n",
    "            color=color, \n",
    "            linewidth=0.8, \n",
    "            label=label\n",
    "        )\n",
    "        label = None\n",
    "\n",
    "    axs[xy].legend()\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12.8, 9.6))\n",
    "\n",
    "for xy in frames:\n",
    "    plot(axs, xy, *frames[xy])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO discuss, what do we see?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsksKs4e4QJA"
   },
   "source": [
    "### Forces Comparison\n",
    "\n",
    "Next, we compute the forces the approaches have generated for the test set trajectories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "me4rb-_qCXgC"
   },
   "outputs": [],
   "source": [
    "gt_forces = utils.infer_forces_sum_from_frames(\n",
    "    gt_frames, domain, diffusion_substeps, viscosity, dt\n",
    ")\n",
    "cfe_forces = utils.infer_forces_sum_from_frames(\n",
    "    cfe_frames, domain, diffusion_substeps, viscosity, dt\n",
    ")\n",
    "rl_forces = rl_trainer.infer_test_set_forces()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B75xKFuw4414"
   },
   "source": [
    "\n",
    "**TODO compute the sum of forces for all test scenes for RL and DP, compare to ground truth values?**\n",
    "\n",
    "In the following, the forces generated by both methods are also visually compared to the ground truth of the respective sample. Dots placed above the blue line denote stronger forces in the analyzed deep learning approach than in the ground truth and vice versa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMK29jgWDUB_"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.8, 9.6))\n",
    "plt.scatter(gt_forces, cfe_forces, label='CFE')\n",
    "plt.scatter(gt_forces, rl_forces, label='RL')\n",
    "plt.plot([x * 100 for x in range(15)], [x * 100 for x in range(15)], label='Same forces as original')\n",
    "plt.xlabel('ground truth')\n",
    "plt.xlim(0, 1500)\n",
    "plt.ylim(0, 1500)\n",
    "plt.ylabel('reconstruction')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ema5wsX25gS6"
   },
   "source": [
    "Next, the two deep learning methods are compared directly. Dots above the line denote higher forces by the control force estimator, samples below higher forces for the reinforcement learning agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEgGuyhcDkPK"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.8, 9.6))\n",
    "plt.scatter(rl_forces, cfe_forces)\n",
    "plt.xlabel('Reinforcement Learning')\n",
    "plt.ylabel('Control Force Estimator')\n",
    "plt.plot([x * 100 for x in range(15)], [x * 100 for x in range(15)], label='Same forces cfe rl')\n",
    "plt.xlim(0, 1500)\n",
    "plt.ylim(0, 1500)\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JqW7Cca6HUJ"
   },
   "source": [
    "The following plot displays the performance of all reinforcement learning, control force estimator and ground truth with respect to individual samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0vqJmQ3FDnKA"
   },
   "outputs": [],
   "source": [
    "w=0.25\n",
    "plot_count=20\n",
    "plt.figure(figsize=(12.8, 9.6))\n",
    "plt.bar(\n",
    "    [i - w for i in range(plot_count)], \n",
    "    rl_forces[:plot_count], \n",
    "    width=w, \n",
    "    align='center', \n",
    "    label='RL'\n",
    ")\n",
    "plt.bar(\n",
    "    [i + w for i in range(plot_count)], \n",
    "    cfe_forces[:plot_count], \n",
    "    width=w, \n",
    "    align='center', \n",
    "    label='CFE'\n",
    ")\n",
    "plt.bar(\n",
    "    [i for i in range(plot_count)], \n",
    "    gt_forces[:plot_count], \n",
    "    width=w, \n",
    "    align='center', \n",
    "    label='GT'\n",
    ")\n",
    "plt.xlabel('Scenes')\n",
    "plt.xticks(range(plot_count))\n",
    "plt.ylabel('Forces')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ee3Us_hD9nR"
   },
   "source": [
    "## Training Progress Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGBlUpQ271Ww"
   },
   "source": [
    "Although the quality of the control in terms of force magnitudes is the primary goal of the setup above, there are interesting differences in terms of how both methods behave at training time. The main difference of the physics-unaware RL training and the DP approach with tightly coupled solver results in a significantly faster convergence for the latter. I.e., the gradients provided by the numerical solver give a much better learning signal than the undirected exploration of the RL process. The behavior of the RL training, on the other hand, can in part be ascribed to the on-policy nature of training data collection and to the more brute-force natured learning technique.\n",
    "\n",
    "The next cell visualizes the training progress of both methods with respect to iterations and wall time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1ecSuakEAYp"
   },
   "outputs": [],
   "source": [
    "def get_cfe_val_set_forces(experiment_path):\n",
    "    path = os.path.join(experiment_path, 'val_forces.csv')\n",
    "    table = pd.read_csv(path)\n",
    "    return list(table['time']), list(table['epoch']), list(table['forces'])\n",
    "\n",
    "rl_w_times, rl_step_nums, rl_val_forces = rl_trainer.get_val_set_forces_data()\n",
    "cfe_w_times, cfe_epochs, cfe_val_forces = get_cfe_val_set_forces('PDE-Control-RL/networks/cfe-models/bench')\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12.8, 9.6))\n",
    "\n",
    "axs[0].plot(np.array(rl_step_nums), rl_val_forces, label='RL')\n",
    "axs[0].plot(np.array(cfe_epochs), cfe_val_forces, label='CFE')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Forces')\n",
    "axs[0].set_ylim(0, 1500)\n",
    "axs[0].grid()\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(np.array(rl_w_times) / 3600, rl_val_forces, label='RL')\n",
    "axs[1].plot(np.array(cfe_w_times) / 3600, cfe_val_forces, label='CFE')\n",
    "axs[1].set_xlabel('Wall time (hours)')\n",
    "axs[1].set_ylabel('Forces')\n",
    "axs[1].set_ylim(0, 1500)\n",
    "axs[1].grid()\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cesgie63S-qL"
   },
   "source": [
    "\n",
    "To conclude, the PPO reinforcement learning yields slightly inferior quality in comparison to the differentiable physics approach, exerting higher forces. Additionally, the time needed for convergence both in terms of wall time and training iterations is significantly higher in the RL case. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ih2a2rackAPs"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "**TODO, what would be interesting to try out for students / readers with the code above?**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PDE-Control-RL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
