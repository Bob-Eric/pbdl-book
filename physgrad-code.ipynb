{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Example with SIP Training\n",
    "\n",
    "placeholder only, insert more complex SIP example \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dont run !!! import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=''  # set GPUs\n",
    "#tf.config.run_functions_eagerly(True)  # deactivate Tensorflow Tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physics Model\n",
    "\n",
    "Differential equation: Coupled Linear Oscillators with Control term\n",
    "\n",
    "Solver: Runge-Kutta-4 \n",
    "\n",
    "Nx: Number of oscillators,\n",
    "Nt: Number of time evolution steps,\n",
    "dt: Length of one time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nx = 2\n",
    "Nt = 24\n",
    "dt = 0.5\n",
    "\n",
    "def build_laplace(n,boundary='0'):\n",
    "    if n==1:\n",
    "        return np.zeros((1,1),dtype=np.float32)\n",
    "    d1 = -2 * np.ones((n,),dtype=np.float32)\n",
    "    d2 = 1 * np.ones((n-1,),dtype=np.float32)\n",
    "    lap = np.zeros((n,n),dtype=np.float32)\n",
    "    lap[range(n),range(n)]=d1\n",
    "    lap[range(1,n),range(n-1)]=d2\n",
    "    lap[range(n-1),range(1,n)]=d2\n",
    "    if boundary=='0':\n",
    "        lap[0,0]=lap[n-1,n-1]=-1\n",
    "\n",
    "    return lap\n",
    "\n",
    "@tf.function\n",
    "def coupled_oscillators_batch( x, control):\n",
    "    print('coup harm osc tracing')\n",
    "    '''\n",
    "    ODE of type:    x' = f(x)\n",
    "    :param x_in:    shape: (batch, 2 * number of osc)\n",
    "                    order second index: x_i , v_i\n",
    "    :param control: shape:(batch,)\n",
    "    :return:\n",
    "    '''\n",
    "    n_osc = x.shape[1]//2\n",
    "\n",
    "    # natural time evo\n",
    "    a1 = np.array([[0,1],[-1,0]],dtype=np.float32)\n",
    "    a2 = np.eye(n_osc,dtype=np.float32)\n",
    "    A = np.kron(a1,a2)\n",
    "    x_dot1 = tf.tensordot(x,A,axes = (1,1))\n",
    "\n",
    "    # interaction term\n",
    "    interaction_strength = 0.2\n",
    "    b1 = np.array([[0,0],[1,0]],dtype=np.float32)\n",
    "    b2 = build_laplace(n_osc)\n",
    "    B = interaction_strength * np.kron(b1,b2)\n",
    "    x_dot2 = tf.tensordot(x,B, axes=(1, 1))\n",
    "\n",
    "    # control term\n",
    "    control_vector = np.zeros((n_osc,),dtype=np.float32)\n",
    "    control_vector[-1] = 1.0\n",
    "    c1 = np.array([0,1],dtype=np.float32)\n",
    "    c2 = control_vector\n",
    "    C = np.kron(c1,c2)\n",
    "    x_dot3 = tf.tensordot(control,C, axes=0)\n",
    "\n",
    "    #all terms\n",
    "    x_dot = x_dot1 + x_dot2 +x_dot3\n",
    "    return x_dot\n",
    "\n",
    "@tf.function\n",
    "def runge_kutta_4_batch(x_0, dt, control, ODE_f_batch):\n",
    "\n",
    "    f_0_0 = ODE_f_batch(x_0, control)\n",
    "    x_14 = x_0 + 0.5 * dt * f_0_0\n",
    "\n",
    "    f_12_14 = ODE_f_batch(x_14, control)\n",
    "    x_12 = x_0 + 0.5 * dt * f_12_14\n",
    "\n",
    "    f_12_12 = ODE_f_batch(x_12, control)\n",
    "    x_34 = x_0 + dt * f_12_12\n",
    "\n",
    "    terms = f_0_0 + 2 * f_12_14 + 2 * f_12_12 + ODE_f_batch(x_34, control)\n",
    "    x1 = x_0 + dt * terms / 6\n",
    "\n",
    "    return x1\n",
    "\n",
    "@tf.function\n",
    "def solver(x0, control):\n",
    "    x = x0\n",
    "    for i in range(Nt):\n",
    "        x = runge_kutta_4_batch(x, dt, control[:,i], coupled_oscillators_batch)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = tf.keras.activations.tanh\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(2*Nx)),\n",
    "    tf.keras.layers.Dense(20, activation=act),\n",
    "    tf.keras.layers.Dense(20, activation=act),\n",
    "    tf.keras.layers.Dense(20, activation=act),\n",
    "    tf.keras.layers.Dense(Nt, activation='linear')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def loss_function(a,b):\n",
    "    diff = a-b\n",
    "    loss_batch = tf.reduce_sum(diff**2,axis=1)\n",
    "    loss = tf.reduce_sum(loss_batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2**12\n",
    "x_train = np.random.rand(N, 2 * Nx).astype(np.float32)\n",
    "y_train = x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "For the optimization procedure, we need to set up some parameters. \n",
    "1. Adam: The truncation parameter has no meaning here.\n",
    "2. PG: The specified optimizer is the one used for network optimization. The physics inversion is done via Gauss-Newton and corresponds to an exact inversion since the physical optimization landscape is quadratic. For the Jacobian inversion in Gauss-Newton, we can specify a truncation parameter.\n",
    "3. HIG: To receive the HIG algorithm, the optimizer has to be set to SGD. For the Jacobian half-inversion, we can specify a truncation parameter. Optimal batch sizes are typically lower. Learning rates can usually be chosen around 1.\n",
    "\n",
    "Specify the maximal simulation time in seconds via max_number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam Training\n",
    "if 0:\n",
    "    mode = 'GD' \n",
    "    optimizer = tf.keras.optimizers.Adam\n",
    "    batch_size = 256\n",
    "    learning_rate = 0.001\n",
    "    max_number = 30#*60\n",
    "    trunc = 10**-10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PG training\n",
    "if 0:\n",
    "    mode = 'PG' \n",
    "    optimizer = tf.keras.optimizers.Adam\n",
    "    batch_size = 256\n",
    "    learning_rate = 0.001\n",
    "    max_number = 30#*60\n",
    "    trunc = 10**-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIG training\n",
    "if 1:\n",
    "    mode = 'HIG' # GD, PG, HIG\n",
    "    optimizer = tf.keras.optimizers.SGD\n",
    "    batch_size = 32\n",
    "    learning_rate = 1.0\n",
    "    max_number = 30#*60\n",
    "    trunc = 10**-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to construct the half-inverse of matrix for HIGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.util import dispatch\n",
    "from tensorflow.python.util.tf_export import tf_export\n",
    "from tensorflow.python.ops.linalg.linalg_impl import _maybe_validate_matrix,svd\n",
    "\n",
    "@tf_export('linalg.gpinv')\n",
    "@dispatch.add_dispatch_support\n",
    "def gpinv(a, rcond=None,beta=0.5, validate_args=False, name=None):\n",
    "\n",
    "  with ops.name_scope(name or 'pinv'):\n",
    "    a = ops.convert_to_tensor(a, name='a')\n",
    "\n",
    "    assertions = _maybe_validate_matrix(a, validate_args)\n",
    "    if assertions:\n",
    "      with ops.control_dependencies(assertions):\n",
    "        a = array_ops.identity(a)\n",
    "\n",
    "    dtype = a.dtype.as_numpy_dtype\n",
    "\n",
    "    if rcond is None:\n",
    "\n",
    "      def get_dim_size(dim):\n",
    "        dim_val = tensor_shape.dimension_value(a.shape[dim])\n",
    "        if dim_val is not None:\n",
    "          return dim_val\n",
    "        return array_ops.shape(a)[dim]\n",
    "\n",
    "      num_rows = get_dim_size(-2)\n",
    "      num_cols = get_dim_size(-1)\n",
    "      if isinstance(num_rows, int) and isinstance(num_cols, int):\n",
    "        max_rows_cols = float(max(num_rows, num_cols))\n",
    "      else:\n",
    "        max_rows_cols = math_ops.cast(\n",
    "            math_ops.maximum(num_rows, num_cols), dtype)\n",
    "      rcond = 10. * max_rows_cols * np.finfo(dtype).eps\n",
    "\n",
    "    rcond = ops.convert_to_tensor(rcond, dtype=dtype, name='rcond')\n",
    "\n",
    "    # Calculate pseudo inverse via SVD.\n",
    "    # Note: if a is Hermitian then u == v. (We might observe additional\n",
    "    # performance by explicitly setting `v = u` in such cases.)\n",
    "    [\n",
    "        singular_values,  # Sigma\n",
    "        left_singular_vectors,  # U\n",
    "        right_singular_vectors,  # V\n",
    "    ] = svd(\n",
    "        a, full_matrices=False, compute_uv=True)\n",
    "\n",
    "    # Saturate small singular values to inf. This has the effect of make\n",
    "    # `1. / s = 0.` while not resulting in `NaN` gradients.\n",
    "    cutoff = rcond * math_ops.reduce_max(singular_values, axis=-1)\n",
    "    singular_values = array_ops.where_v2(\n",
    "        singular_values > array_ops.expand_dims_v2(cutoff, -1), singular_values**beta,\n",
    "        np.array(np.inf, dtype))\n",
    "\n",
    "    # By the definition of the SVD, `a == u @ s @ v^H`, and the pseudo-inverse\n",
    "    # is defined as `pinv(a) == v @ inv(s) @ u^H`.\n",
    "    a_pinv = math_ops.matmul(\n",
    "        right_singular_vectors / array_ops.expand_dims_v2(singular_values, -2),\n",
    "        left_singular_vectors,\n",
    "        adjoint_b=True)\n",
    "\n",
    "    if a.shape is not None and a.shape.rank is not None:\n",
    "      a_pinv.set_shape(a.shape[:-2].concatenate([a.shape[-1], a.shape[-2]]))\n",
    "\n",
    "    return a_pinv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a Python class to organize the optimization algorithm. Using physics solver, network model, loss function and a data set. We compute loss values after each epoch. Each epoch consists of several mini batch updates. Depending on the optimization method, the mini batch update differs:\n",
    "1. Adam: Compute loss gradient, then apply the Adam\n",
    "2. PG: Compute loss gradient und physics Jacobian, invert them data-point-wise, define a modified loss to compute network updates\n",
    "3. HIG: Compute loss gradient and network-physics Jacobian, then compute half-inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimization():\n",
    "    def __init__(self,model,solver,loss_function,x_train,y_train):\n",
    "        self.model = model\n",
    "        self.solver = solver\n",
    "        self.loss_function = loss_function\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.y_dim = y_train.shape[1]\n",
    "        self.weight_shapes = [weight_tensor.shape for weight_tensor in self.model.trainable_weights]\n",
    "\n",
    "    def set_params(self,batch_size,learning_rate,optimizer,max_number,mode,trunc):\n",
    "        self.number_of_batches = N // batch_size\n",
    "        self.max_number = max_number\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer(learning_rate)\n",
    "        self.mode = mode\n",
    "        self.trunc = trunc\n",
    "\n",
    "\n",
    "    def computation(self,x_batch, y_batch):\n",
    "        control_batch = self.model(y_batch)\n",
    "        y_prediction_batch = self.solver(x_batch,control_batch)\n",
    "        loss = self.loss_function(y_batch,y_prediction_batch)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def gd_get_derivatives(self,x_batch, y_batch):\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(self.model.trainable_variables)\n",
    "            loss = self.computation(x_batch,y_batch)\n",
    "            loss_per_dp = loss / self.batch_size\n",
    "        grad = tape.gradient(loss_per_dp, self.model.trainable_variables)\n",
    "        return grad\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def pg_get_physics_derivatives(self,x_batch, y_batch): #physical grads\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            control_batch = self.model(y_batch)\n",
    "            tape.watch(control_batch)\n",
    "            y_prediction_batch = self.solver(x_batch,control_batch)\n",
    "            loss = self.loss_function(y_batch,y_prediction_batch)\n",
    "            loss_per_dp = loss / self.batch_size\n",
    "\n",
    "        jacy = tape.batch_jacobian(y_prediction_batch,control_batch)\n",
    "        grad = tape.gradient(loss_per_dp, y_prediction_batch)\n",
    "        return jacy,grad,control_batch\n",
    "\n",
    "    @tf.function\n",
    "    def pg_get_network_derivatives(self,x_batch, y_batch,new_control_batch): #physical grads\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(self.model.trainable_variables)\n",
    "            control_batch = self.model(y_batch)\n",
    "            loss = self.loss_function(new_control_batch,control_batch)\n",
    "            #y_prediction_batch = self.solver(x_batch,control_batch)\n",
    "            #loss = self.loss_function(y_batch,y_prediction_batch)\n",
    "            loss_per_dp = loss / self.batch_size\n",
    "\n",
    "\n",
    "\n",
    "        network_grad = tape.gradient(loss_per_dp, self.model.trainable_variables)\n",
    "        return network_grad\n",
    "\n",
    "    @tf.function\n",
    "    def hig_get_derivatives(self,x_batch,y_batch):\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(self.model.trainable_variables)\n",
    "            control_batch = self.model(y_batch)\n",
    "            y_prediction_batch = self.solver(x_batch,control_batch)\n",
    "            loss = self.loss_function(y_batch,y_prediction_batch)\n",
    "            loss_per_dp = loss / self.batch_size\n",
    "\n",
    "        jacy = tape.jacobian(y_prediction_batch, self.model.trainable_variables, experimental_use_pfor=True)\n",
    "        loss_grad = tape.gradient(loss_per_dp, y_prediction_batch)\n",
    "        return jacy, loss_grad\n",
    "\n",
    "\n",
    "\n",
    "    def mini_batch_update(self,x_batch, y_batch):\n",
    "        if self.mode==\"GD\":\n",
    "            grad = self.gd_get_derivatives(x_batch, y_batch)\n",
    "            self.optimizer.apply_gradients(zip(grad, self.model.trainable_weights))\n",
    "\n",
    "        elif self.mode==\"PG\":\n",
    "            jacy,grad,control_batch = self.pg_get_physics_derivatives(x_batch, y_batch)\n",
    "            grad_e = tf.expand_dims(grad,-1)\n",
    "            pinv = tf.linalg.pinv(jacy,rcond=10**-5)\n",
    "            delta_control_label_batch = (pinv@grad_e)[:,:,0]\n",
    "            new_control_batch = control_batch - delta_control_label_batch\n",
    "            network_grad = self.pg_get_network_derivatives(x_batch, y_batch,new_control_batch)\n",
    "            self.optimizer.apply_gradients(zip(network_grad, self.model.trainable_weights))\n",
    "\n",
    "        elif self.mode =='HIG':\n",
    "            jacy, grad = self.hig_get_derivatives(x_batch, y_batch)\n",
    "            flat_jacy_list = [tf.reshape(jac, (self.batch_size * self.y_dim, -1)) for jac in jacy]\n",
    "            flat_jacy = tf.concat(flat_jacy_list, axis=1)\n",
    "            flat_grad = tf.reshape(grad, (-1,))\n",
    "            inv = gpinv(flat_jacy, rcond=self.trunc)\n",
    "            processed_derivatives = tf.tensordot(inv, flat_grad, axes=(1, 0))\n",
    "            #processed_derivatives = self.linear_solve(flat_jacy, flat_grad)\n",
    "            update_list = []\n",
    "            l1 = 0\n",
    "            for k, shape in enumerate(self.weight_shapes):\n",
    "                l2 = l1 + np.prod(shape)\n",
    "                upd = processed_derivatives[l1:l2]\n",
    "                upd = np.reshape(upd, shape)\n",
    "                update_list.append(upd)\n",
    "                l1 = l2\n",
    "            self.optimizer.apply_gradients(zip(update_list, self.model.trainable_weights))\n",
    "\n",
    "\n",
    "    def epoch_update(self):\n",
    "        for batch_index in range(self.number_of_batches):\n",
    "            position = batch_index * batch_size\n",
    "            x_batch = self.x_train[position:position + batch_size]\n",
    "            y_batch = self.y_train[position:position + batch_size]\n",
    "            self.mini_batch_update(x_batch, y_batch)\n",
    "\n",
    "    def eval(self,epoch,wc_time,ep_dur):\n",
    "\n",
    "        print('Epoch: ', epoch,' WallClockTime: ',wc_time,' EpochDuration: ',ep_dur )\n",
    "\n",
    "        train_loss = self.computation(self.x_train,self.y_train)\n",
    "        train_loss_per_dp = train_loss / N\n",
    "        print('TrainLoss:', train_loss_per_dp)\n",
    "        return train_loss_per_dp\n",
    "\n",
    "    def start_training(self):\n",
    "        init_loss = self.eval(0,0,0)\n",
    "        init_time = time.time()\n",
    "        time_list = [init_time]\n",
    "        loss_list = [init_loss]\n",
    "\n",
    "        epoch=0\n",
    "        wc_time = 0\n",
    "\n",
    "        while wc_time<max_number:\n",
    "            \n",
    "            duration = time.time()\n",
    "            self.epoch_update()\n",
    "            duration = time.time()-duration\n",
    "\n",
    "            epoch += 1\n",
    "            wc_time += duration\n",
    "\n",
    "            loss = self.eval(epoch,wc_time,duration)\n",
    "            time_list.append(duration)\n",
    "            loss_list.append(loss)\n",
    "\n",
    "        time_list = np.array(time_list)\n",
    "        loss_list = np.array(loss_list)\n",
    "        time_list[0] = 0\n",
    "        return time_list, loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  WallClockTime:  0  EpochDuration:  0\n",
      "coup harm osc tracing\n",
      "TrainLoss: tf.Tensor(1.0081012, shape=(), dtype=float32)\n",
      "coup harm osc tracing\n",
      "Epoch:  1  WallClockTime:  27.670811891555786  EpochDuration:  27.670811891555786\n",
      "TrainLoss: tf.Tensor(6.0117705e-05, shape=(), dtype=float32)\n",
      "Epoch:  2  WallClockTime:  30.26737689971924  EpochDuration:  2.596565008163452\n",
      "TrainLoss: tf.Tensor(1.1014066e-05, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "opt = Optimization(model,solver,loss_function,x_train,y_train)\n",
    "opt.set_params(batch_size,learning_rate,optimizer,max_number,mode,trunc)\n",
    "time_list, loss_list = opt.start_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot and save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['HIG'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wct_times = {}\n",
    "wct_times[mode] = wct_time\n",
    "print(format(wct_times.keys()))\n",
    "\n",
    "wct_time = np.cumsum(time_list)\n",
    "plt.plot(wct_time,loss_list)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Wall clock time')\n",
    "plt.xlabel('Loss')\n",
    "plt.title('Linear Chain - '+mode+'_'+str(optimizer.__name__))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './'\n",
    "name1 = 'time'+mode\n",
    "name2 = 'loss'+mode\n",
    "np.savetxt(path+name1+'.txt',time_list)\n",
    "np.savetxt(path+name2+'.txt',loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60a46218545510a486eb79ffac0761a327b44b02462576ac242e6c15f2abc4b7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
